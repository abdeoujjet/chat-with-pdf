{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85360ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  \n",
    "from openai import OpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2061f0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aoujjet\\OneDrive - Grupo Sampol\\Escritorio\\Proyectos LLM\\ChatWithPDF\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6b6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c4d899",
   "metadata": {},
   "source": [
    "Subir y leer PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5c8f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En este archivo se recopilan todos los pasos y modificaciones necesarias para \n",
      "poner en marcha un GD desde cero. \n",
      "**Nota: Leer el archivo completo antes de comenzar a implementar** \n",
      "Configuración del Servidor Ubuntu (si es necesario) \n",
      "1. Instalación de Programas: \n",
      "• \n",
      "Se deben instalar los programas necesarios en el servidor Ubuntu \n",
      "según los requisitos del proyecto. \n",
      "2. Localización de la Tarjeta de Red: \n",
      "• \n",
      "Se debe identificar y localizar la tarjeta de red del servidor para \n",
      "configurar la conex\n"
     ]
    }
   ],
   "source": [
    "def cargar_pdf(path_pdf):\n",
    "    texto_completo = \"\"\n",
    "    doc = fitz.open(path_pdf)\n",
    "    for pagina in doc:\n",
    "        texto_completo += pagina.get_text()\n",
    "    doc.close()\n",
    "    return texto_completo\n",
    "\n",
    "# Simulamos la subida\n",
    "pdf_path = r\"Produccion_Nuevo_Gemelo.pdf\"  # Debes tener este archivo en el mismo directorio que tu notebook\n",
    "texto_pdf = cargar_pdf(pdf_path)\n",
    "\n",
    "# Mostramos los primeros 500 caracteres\n",
    "print(texto_pdf[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2def68",
   "metadata": {},
   "source": [
    "dividir texto en chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24263397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el texto en documentos\n",
    "documento = Document(page_content=texto_pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54458c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=400)\n",
    "chunks = text_splitter.split_documents([documento])\n",
    "len(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d7cfbc",
   "metadata": {},
   "source": [
    "Embedding y vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc05d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"ChatVectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df0a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoujjet\\AppData\\Local\\Temp\\ipykernel_7780\\102896465.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3715d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaciar la BD si existe \n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5830e1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 8 documents\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53c47b",
   "metadata": {},
   "source": [
    "Modelo LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ada18f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoujjet\\AppData\\Local\\Temp\\ipykernel_7780\\641253344.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=MODEL)\n",
      "C:\\Users\\aoujjet\\AppData\\Local\\Temp\\ipykernel_7780\\641253344.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gemma3:4b\" \n",
    "\n",
    "# 1) llm\n",
    "llm = ChatOllama(model=MODEL)\n",
    "\n",
    "# 2) set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# 3) the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 4) putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a185fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aoujjet\\AppData\\Local\\Temp\\ipykernel_7780\\2065357034.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents(\"172.30.10.152:9443\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='fe38605e-8178-48cf-8e1c-9730670a5f50', metadata={}, page_content='• \\nAñadir DB.cfg de la API (donde esta DB_template.cfg)  \\n• \\nAñadir en scheduler, mongo.cfg y añadir carpeta taglists en \\nscheduler de la API \\n• \\nModificar la IP del backfill en producer/backfill_stream.py \\n➢ Arrancar el productor Kafka \\n➢ Arrancar la API docker tween-streaming-api -d en \\nTweenStremingApi \\n➢ Arrancar docker completo de TweenStremingApi (para ver \\nerrores) \\n➢ Abrir portainer para ver que todo ha ido bien: 172.30.10.152:9443 \\npara ver logs y reiniciar contenedores etc. \\n➢ Abrir Kafka para ver si llegan mensajes 172.30.10.152:8080/ \\n \\n4. Modificar en GDDataPlataform, el repositorio TweenDataLake: \\n• \\nCrear la base de datos en Dbeaver (Timescale).  \\n1. Host 172.30.10.249 \\n2. port 5432 \\n3. usr y pw: root  \\n4. Nombre de la DB: el nombre de la central igual que en \\nmongo. \\n• \\nAñadir archivo db/init.sql. Editarlo y añadir las 3 filas que hay para \\ncada GD.  \\n• \\nEn TweenDataLake/consumer/config.cfg añadir el nuevo GD con la'),\n",
       " Document(id='757d49b6-6bac-4924-8a54-735c873f6766', metadata={}, page_content='• \\nModificar strat_backfill.py añadiendo el GD en la configuración \\n• \\nModificar train_flow_model.py añadiendo el GD en la \\nconfiguración.  \\n• \\nProbar el orquestador en local. Para ello hacer un dokcer compose \\nup únicamente, el resto de las actualizaciones del código se \\nejecutarán simultáneamente. \\n• \\nUna vez funcione, subirlo a Git y hacer un pull en GDdataplatform.  \\n• \\nSubir los nuevos modelos a GDdataplatform y su api (equivalente a \\nParcBit_Models)'),\n",
       " Document(id='91dd3d10-ff53-41cc-bf84-90540a223304', metadata={}, page_content='4. Modificar en GDDataPlataform, el repositorio TweenDataLake: \\n• \\nCrear la base de datos en Dbeaver (Timescale).  \\n1. Host 172.30.10.249 \\n2. port 5432 \\n3. usr y pw: root  \\n4. Nombre de la DB: el nombre de la central igual que en \\nmongo. \\n• \\nAñadir archivo db/init.sql. Editarlo y añadir las 3 filas que hay para \\ncada GD.  \\n• \\nEn TweenDataLake/consumer/config.cfg añadir el nuevo GD con la \\nIP y el nombre de la Planta. Mas abajo añadir la central en \\n[BDmongo].  \\n5. Otros: \\n• \\nComando matar/eliminar topics creados erroneamente: kafka-topics --\\nbootstrap-server localhost:9092 --delete --topic Hyatt.Chillers                 \\n \\n \\nParte de Cloud \\n1. Clonar el repositorio TweenOfflineDataOrchestrator: \\n2. Configuración de Nuevos Modelos: \\n• \\nSi es subir nuevos modelos, añadirlos en el dag existente.  \\n• \\nSi es nuevo gemelo, en custom_dags crear un nuevo archivo .py \\npara la central, ej: models_hyatt.py (copiando el que ya existe). En \\neste archivo: \\n1. Primero crear backfill'),\n",
       " Document(id='95332a0f-dbfb-41a5-ba84-3a54a4925aff', metadata={}, page_content='En este archivo se recopilan todos los pasos y modificaciones necesarias para \\nponer en marcha un GD desde cero. \\n**Nota: Leer el archivo completo antes de comenzar a implementar** \\nConfiguración del Servidor Ubuntu (si es necesario) \\n1. Instalación de Programas: \\n• \\nSe deben instalar los programas necesarios en el servidor Ubuntu \\nsegún los requisitos del proyecto. \\n2. Localización de la Tarjeta de Red: \\n• \\nSe debe identificar y localizar la tarjeta de red del servidor para \\nconfigurar la conexión de red correctamente. \\n3. Asignación de IP (DHCP True): \\n• \\nLa asignación de una dirección IP al servidor debe configurarse con \\nDHCP activado, si corresponde según la infraestructura de red. \\n4. Instalación de Docker y Configuración de Permisos (chmod): \\n• \\nDocker debe ser instalado en el servidor y se deben configurar los \\npermisos adecuados, utilizando el comando chmod, para garantizar \\nsu correcto funcionamiento. \\n5. Instalación de Clave de Git: \\n•')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"172.30.10.152:9443\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683547a",
   "metadata": {},
   "source": [
    "Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74cb1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4101fd7",
   "metadata": {},
   "source": [
    "Gradio IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f8cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be31f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear el interfaz de chat\n",
    "view = gr.ChatInterface(chat, type=\"messages\")\n",
    "\n",
    "# Lanza el servidor en 0.0.0.0:7860 para que esté accesible externamente\n",
    "view.launch(\n",
    "    inbrowser=False,\n",
    "    share=False,\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7860\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
